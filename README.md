This repo is a hobby project where I explore the usage of the Mamba architecture used in conjunction with transformers for training LLMs. 
The Mamba architecture is a selective state space mechanism that using convolutions to create a sort of self-attention mechanism found in transformers.
Mamba blocks are advantageous because they require less parameters to function saving memory, have a linear time complexity, and when used with transformers, offer equivalent performance. 


## Acknowledgements

This project uses code from the following repositories:

1. **build-nanogpt** by Andrej Karpathy (https://github.com/karpathy)
    - Repository URL: [https://github.com/karpathy/build-nanogpt]
    - License: MIT
2. **mamba** by Tri Dao, Albert Gu (https://github.com/state-spaces)
    - Repository URL: [https://github.com/state-spaces/mamba]
    - License: Apache


-README.me
    This file.

